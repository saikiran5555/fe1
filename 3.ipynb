{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d21ea965",
   "metadata": {},
   "source": [
    "Embedded feature selection methods integrate the feature selection process as part of the model training. They are called \"embedded\" because the feature selection occurs naturally during the model learning process. Unlike filter and wrapper methods, embedded methods take into account the interaction between features and the model. Here are some common techniques used in embedded feature selection methods:\n",
    "\n",
    "Lasso Regression (L1 Regularization):\n",
    "\n",
    "In Lasso regression, the L1 penalty is applied to the regression coefficients. This method can shrink some coefficients to zero, effectively selecting a subset of features that contribute most to the target variable.\n",
    "Ridge Regression (L2 Regularization):\n",
    "\n",
    "Although Ridge regression doesn’t perform feature selection in the strictest sense (it doesn’t set coefficients to zero), it is often grouped with embedded methods. It applies an L2 penalty to reduce model complexity and prevent overfitting.\n",
    "Elastic Net:\n",
    "\n",
    "Elastic Net is a middle ground between Lasso and Ridge. It combines both L1 and L2 regularization, which can be beneficial in scenarios where there are multiple features that are correlated with each other.\n",
    "Decision Trees:\n",
    "\n",
    "Decision tree algorithms like CART, C4.5, and Random Forest inherently perform feature selection by selecting the most informative features for splitting the nodes. Features that are not used in splits are effectively ignored by the model.\n",
    "Gradient Boosting Machines (GBMs):\n",
    "\n",
    "Algorithms like XGBoost, LightGBM, and Gradient Boosting Classifier can also be considered embedded methods. They assign importance to features based on how useful they are at reducing variance or impurity.\n",
    "Regularized Random Forests:\n",
    "\n",
    "This is a variation of the standard Random Forest with an added regularization step to select a subset of features.\n",
    "Support Vector Machines (SVM) with Kernel Methods:\n",
    "\n",
    "While not a feature selection method in the traditional sense, SVMs with kernels can highlight feature importance depending on their impact on the hyperplane's decision boundary."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
